# -*- coding: utf-8 -*-
"""Ramen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18SztjV8st_ldSjNKiv_Dcwc1AbV67DXr

# Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

import warnings
warnings.filterwarnings('ignore')

"""# Data Loading

Langkah ini bertujuan untuk memuat dataset yang diperlukan untuk membangun sistem rekomendasi.
"""

# Path dataset yang sudah kamu upload ke Colab
file_path = '/content/ramen-ratings.csv'

# Membaca dataset CSV menggunakan pandas
df = pd.read_csv(file_path)

# Menampilkan ukuran dataset dan 5 baris pertama sebagai gambaran awal
print("Ukuran dataset:", df.shape)
print(df.head())

"""#  Data Understanding.

Pada tahap ini awal dalam proses analisis data yang bertujuan untuk mengenal dan memahami struktur, tipe, serta kualitas data ramen yang digunakan dalam penelitian. Pada tahap ini, dilakukan pemeriksaan terhadap jumlah data, tipe variabel, serta kelengkapan data termasuk pengecekan nilai yang hilang (missing values).
"""

# Menampilkan info struktur dataset (tipe data dan non-null count)
print("Info dataset:")
print(df.info())

# Menampilkan statistik deskriptif untuk kolom numerik dan kategori
print("\nStatistik deskriptif:")
print(df.describe(include='all'))

# Cek missing value di tiap kolom
print("\nJumlah missing values per kolom:")
print(df.isnull().sum())

"""# Exploratory Data Analysis (EDA)

Melalui tahap ini, dilakukan analisis distribusi rating (Stars) untuk mengetahui sebaran preferensi pengguna terhadap produk ramen. Selain itu, dilakukan identifikasi brand-brand dengan rata-rata rating tertinggi guna memberikan gambaran merek mana yang memiliki kualitas unggul menurut data rating.karakteristik dan pola dalam dataset
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Distribusi rating (Stars)
plt.figure(figsize=(8,5))
sns.countplot(data=df, x='Stars', palette='viridis')
plt.title('Distribusi Rating Ramen (Stars)')
plt.xlabel('Rating (Stars)')
plt.ylabel('Jumlah Data')
plt.show()

# Rating rata-rata per Brand (top 10 brand dengan rating tertinggi)
avg_rating_brand = df.groupby('Brand')['Stars'].mean().sort_values(ascending=False).head(10)

plt.figure(figsize=(10,6))
sns.barplot(x=avg_rating_brand.values, y=avg_rating_brand.index, palette='magma')
plt.title('Top 10 Brand dengan Rating Ramen Tertinggi (Rata-rata)')
plt.xlabel('Rata-rata Rating (Stars)')
plt.ylabel('Brand')
plt.show()

print("Top 10 Brand dengan Rating Tertinggi:")
print(avg_rating_brand)

"""# Data Preparation

Tahap Data Preparation ini memastikan dataset dalam kondisi bersih, lengkap, dan sesuai dengan kebutuhan algoritma rekomendasi yang akan digunakan pada tahap berikutnya, baik content-based maupun collaborative filtering.
"""

df = df.drop(columns=['Top Ten'])
print("Kolom 'Top Ten' sudah dihapus.")

# Ubah 'Stars' ke numeric, dengan errors='coerce' agar nilai yang tidak bisa diubah jadi NaN
df['Stars'] = pd.to_numeric(df['Stars'], errors='coerce')

# Isi missing di kolom 'Style' dengan 'Unknown'
df['Style'] = df['Style'].fillna('Unknown')

# Cek kembali jumlah missing setelah perubahan
print("Missing values setelah preprocessing:")
print(df.isnull().sum())

# Buang baris dengan nilai Stars yang kosong
df = df.dropna(subset=['Stars'])

# Cek kembali ukuran data setelah drop
print("Ukuran data setelah menghapus baris tanpa rating:", df.shape)

# Pastikan Stars sudah numeric dan tidak ada missing
print("Jumlah missing values pada Stars:", df['Stars'].isnull().sum())

# Isi missing value kolom 'Country' jika ada
df['Country'] = df['Country'].fillna('Unknown')

# Cek kolom 'Country' sebagai proxy user
print("Jumlah data unik Country (sebagai user):", df['Country'].nunique())

# Contoh tampilkan beberapa data
print(df[['Country', 'Brand', 'Variety', 'Stars']].head())

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(df['Variety'])

print("Shape TF-IDF matrix:", tfidf_matrix.shape)

"""# Model Development dengan Content-Based Filtering

Pada tahap Model Content-Based Filtering menggunakan TF-IDF untuk merepresentasikan teks ramen dan cosine similarity untuk mengukur kemiripan antar ramen. Fungsi rekomendasi mengembalikan 10 ramen paling mirip berdasarkan kemiripan konten, memudahkan pengguna menemukan ramen serupa tanpa data preferensi pengguna.

## Cosine Similarity

Mengukur kemiripan antar setiap pasangan ramen menggunakan matriks TF-IDF yang dihasilkan.
"""

from sklearn.metrics.pairwise import cosine_similarity

# Hitung cosine similarity antar baris TF-IDF matrix
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Cek ukuran matrix similarity
print("Shape cosine similarity matrix:", cosine_sim.shape)

"""# Evaluasi Content-Based Filtering
Precisionk mengukur proporsi item relevan dalam rekomendasi teratas. Dalam konteks ini, kita bisa mendefinisikan "relevan" sebagai ramen yang memiliki rating tinggi (misalnya, di atas 4 bintang).
"""

def recommend_ramen_content_based(title, cosine_sim=cosine_sim, df=df):
    # Membuat indeks dari kolom 'Variety' untuk pencarian
    indices = pd.Series(df.index, index=df['Variety']).drop_duplicates()

    if title not in indices:
        return f"Ramen dengan nama '{title}' tidak ditemukan dalam data."

    # Ambil indeks. Jika indices[title] mengembalikan Series (karena duplikasi nama), ambil indeks pertama.
    idx = indices[title]
    if isinstance(idx, pd.Series):
        idx = idx.iloc[0] # Ambil indeks pertama jika multiple matches

    # Ambil baris similarity untuk ramen ini, pastikan 1D array
    sim_scores_array = cosine_sim[idx]

    # Kalau sim_scores_array bentuknya 2D, flatten dulu
    if sim_scores_array.ndim > 1:
        sim_scores_array = sim_scores_array.flatten()

    # Buat list tuple (index, similarity_score)
    sim_scores = list(enumerate(sim_scores_array))

    # Sort berdasarkan similarity descending, ambil 10 teratas kecuali diri sendiri
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Hapus diri sendiri (indeks idx)
    sim_scores = [x for x in sim_scores if x[0] != idx]

    # Ambil 10 teratas
    sim_scores = sim_scores[:10]

    ramen_indices = [i[0] for i in sim_scores]

    recommended = df.iloc[ramen_indices][['Brand', 'Variety', 'Style', 'Country', 'Stars']]

    return recommended.reset_index(drop=True)

# Hitung Precision@10 untuk model content-based
precision_at_10_cb = evaluate_content_based(recommend_ramen_content_based, df, k=10, relevant_threshold=4.0)
print(f"Precision10 (Content-Based Filtering): {precision_at_10_cb:.4f}")

"""## Buat Indeks Nama Ramen"""

print(df['Variety'].unique())

print(df['Variety'].head(10))

# Membuat indeks dari kolom 'Variety' untuk pencarian
indices = pd.Series(df.index, index=df['Variety']).drop_duplicates()

# Contoh cek indeks ramen 'Shin Black'
print("Indeks ramen 'Singapore Curry':", indices['Singapore Curry'])

result = recommend_ramen_content_based('Singapore Curry')
print(result)

"""# Model Development dengan Collaborative Filtering

## Membagi data menjadi training dan testing
 kita bagi data menjadi training (80%) dan testing (20%) dengan stratified sampling berdasarkan user agar distribusi rating tetap merata.
"""

train_data, test_data = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Country_mod'])

"""## Membuat matriks user-item

Pada tahap ini, kita mengembangkan model rekomendasi dengan pendekatan Collaborative Filtering menggunakan kolom Country sebagai proxy pengguna untuk membuat matriks user-item berdasarkan rating ramen. Kemiripan antar pengguna dihitung dengan cosine similarity, lalu rekomendasi dibuat dengan memanfaatkan preferensi pengguna serupa. Pendekatan ini memungkinkan sistem memprediksi ramen yang disukai pengguna berdasarkan pola rating kolektif dalam data.
"""

# Membuat kolom modifikasi Country
country_counts = df['Country'].value_counts()
rare_countries = country_counts[country_counts < 2].index.tolist()
df['Country_mod'] = df['Country'].apply(lambda x: 'Others' if x in rare_countries else x)

country_counts = df['Country'].value_counts()
rare_countries = country_counts[country_counts < 2].index.tolist()
df['Country_mod'] = df['Country'].apply(lambda x: 'Others' if x in rare_countries else x)

train_data, test_data = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Country_mod'])
print("Data training shape:", train_data.shape)
print("Data testing shape:", test_data.shape)

# Membuat matriks user-item dari data training
train_matrix = train_data.pivot_table(index='Country', columns='Variety', values='Stars')

print("Shape matriks training user-item:", train_matrix.shape)
train_matrix.head()

"""## Hitung similarity antar user
Untuk membuat rekomendasi berbasis user (User-Based Collaborative Filtering), kita hitung similarity antar user menggunakan cosine similarity dengan mengisi nilai NaN menjadi 0.
"""

from sklearn.metrics.pairwise import pairwise_distances
import numpy as np

# Isi NaN dengan 0 untuk hitung similarity
train_matrix_filled = train_matrix.fillna(0)

# Hitung cosine similarity antar user (Country)
user_similarity = 1 - pairwise_distances(train_matrix_filled, metric='cosine')

print("Shape matriks similarity user:", user_similarity.shape)

"""# Evaluasi Collaborative Filtering

Model collaborative filtering memprediksi rating yang mungkin diberikan pengguna pada ramen yang belum mereka coba. Metrik evaluasi yang umum untuk tugas prediksi rating adalah Root Mean Squared Error (RMSE) dan Mean Absolute Error (MAE). Kita akan menghitung ini pada data test_data.
"""

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

def predict_collaborative(user_id, item_variety, user_similarity, train_matrix):

    if user_id not in train_matrix.index or item_variety not in train_matrix.columns:
        return np.nan # Tidak bisa prediksi jika user atau item tidak ada di training

    user_idx = train_matrix.index.get_loc(user_id)
    item_idx = train_matrix.columns.get_loc(item_variety)

    # Ambil skor similarity user terhadap user lain
    sim_scores = user_similarity[user_idx]

    # Temukan user yang juga pernah merating item ini
    users_who_rated_item = train_matrix[item_variety].dropna().index.tolist()

    if not users_who_rated_item:
        return np.nan # Tidak ada user yang pernah merating item ini

    # Filter user similarity hanya untuk user yang merating item ini
    users_to_consider = [u for u in users_who_rated_item if u in train_matrix.index]
    if not users_to_consider:
        return np.nan

    # Dapatkan indeks dari user yang dipertimbangkan
    users_to_consider_indices = [train_matrix.index.get_loc(u) for u in users_to_consider]

    # Ambil skor similarity user_id terhadap user-user yang dipertimbangkan
    relevant_sim_scores = sim_scores[users_to_consider_indices]

    # Ambil rating dari user-user yang dipertimbangkan untuk item ini
    relevant_ratings = train_matrix.loc[users_to_consider, item_variety]

    # Hitung prediksi rating
    # Weighted average of ratings by similar users
    # Handle case where sum of absolute similarities is 0
    if np.sum(np.abs(relevant_sim_scores)) == 0:
        return np.nan # Tidak bisa menghitung prediksi jika tidak ada similarity

    predicted_rating = np.sum(relevant_sim_scores * relevant_ratings) / np.sum(np.abs(relevant_sim_scores))

    return predicted_rating


# Siapkan data test untuk prediksi
y_true = test_data['Stars'].values
y_pred = []
actual_y_true = [] # Simpan nilai true rating yang berhasil diprediksi

# Lakukan prediksi untuk setiap baris di data test
for index, row in test_data.iterrows():
    user = row['Country']
    item = row['Variety']
    true_rating = row['Stars']

    predicted = predict_collaborative(user, item, user_similarity, train_matrix)

    if not np.isnan(predicted):
        y_pred.append(predicted)
        actual_y_true.append(true_rating)


# Hitung RMSE dan MAE
if actual_y_true:
    rmse = np.sqrt(mean_squared_error(actual_y_true, y_pred))
    mae = mean_absolute_error(actual_y_true, y_pred)

    print(f"\nRMSE (Collaborative Filtering - Test Data): {rmse:.4f}")
    print(f"MAE (Collaborative Filtering - Test Data): {mae:.4f}")
else:
    print("\nTidak ada prediksi yang berhasil dilakukan pada data test untuk Collaborative Filtering.")

"""## Fungsi Rekomendasi Collaborative Filtering"""

def recommend_ramen_collaborative(user_id, user_similarity=user_similarity, train_matrix=train_matrix):
    if user_id not in train_matrix.index:
        return f"User '{user_id}' tidak ditemukan dalam data training."

    # Ambil indeks user
    user_idx = train_matrix.index.get_loc(user_id)

    # Ambil skor similarity user terhadap user lain
    sim_scores = list(enumerate(user_similarity[user_idx]))

    # Urutkan berdasarkan similarity tertinggi (kecuali diri sendiri)
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = [x for x in sim_scores if x[0] != user_idx]

    # Ambil 10 user paling mirip
    top_users = [train_matrix.index[i[0]] for i in sim_scores[:10]]

    # Ambil ramen yang user belum rating
    user_rated = train_matrix.loc[user_id].dropna().index.tolist()

    # Rata-rata rating ramen dari user mirip yang belum dicoba user_id
    candidate_ratings = train_matrix.loc[top_users].mean(axis=0).dropna()
    candidate_ratings = candidate_ratings.drop(user_rated, errors='ignore')

    # Urutkan dan ambil 10 rekomendasi teratas
    recommendations = candidate_ratings.sort_values(ascending=False).head(10)

    result = pd.DataFrame({
        'Ramen': recommendations.index,
        'Predicted Rating': recommendations.values
    })

    return result.reset_index(drop=True)

sample_user = train_matrix.index[0]  # ambil user pertama sebagai contoh
print(f"Rekomendasi ramen untuk user '{sample_user}':")
print(recommend_ramen_collaborative(sample_user))